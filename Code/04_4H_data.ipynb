{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import RobustScaler\n",
                "import os\n",
                "import joblib\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "N_STEPS_IN = 30  \n",
                "N_STEPS_OUT = 7 \n",
                "\n",
                "def create_multistep_sequences(data, target, n_steps_in, n_steps_out):\n",
                "    X, y = [], []\n",
                "    if len(data) <= (n_steps_in + n_steps_out):\n",
                "        return np.array([]), np.array([])\n",
                "        \n",
                "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
                "        end_ix = i + n_steps_in\n",
                "        out_end_ix = end_ix + n_steps_out\n",
                "        seq_x = data[i:end_ix, :]\n",
                "        seq_y = target[end_ix:out_end_ix, 0]\n",
                "        X.append(seq_x)\n",
                "        y.append(seq_y)\n",
                "    return np.array(X), np.array(y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data...\n",
                        "Resampling to 4-Hour (4H)...\n",
                        "4H Data Shape: (615, 3)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\V I C T U S\\AppData\\Local\\Temp\\ipykernel_34444\\1370493223.py:15: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
                        "  df_4h = df.resample('4H').agg({\n"
                    ]
                }
            ],
            "source": [
                "# Load Data\n",
                "data_path = 'training_data.csv'\n",
                "if not os.path.exists(data_path):\n",
                "    print(\"Error: training_data.csv not found.\")\n",
                "    raise SystemExit('Stopping execution')\n",
                "\n",
                "print(\"Loading data...\")\n",
                "df = pd.read_csv(data_path)\n",
                "df['datetime'] = pd.to_datetime(df['datetime'])\n",
                "df = df.sort_values('datetime')\n",
                "df = df.set_index('datetime')\n",
                "\n",
                "# Resample to 4-Hour (4H)\n",
                "print(\"Resampling to 4-Hour (4H)...\")\n",
                "df_4h = df.resample('4H').agg({\n",
                "    'price': 'mean',\n",
                "    'volume': 'sum', \n",
                "    'Sentiment Score': 'mean'\n",
                "}).dropna()\n",
                "print(f\"4H Data Shape: {df_4h.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Calculating Rolling Volatility (Regime Proxy)...\n",
                        "Volatility Feature Created.\n"
                    ]
                }
            ],
            "source": [
                "# Regime Detection (ONLINE / NO LEAKAGE)\n",
                "print(\"Calculating Rolling Volatility (Regime Proxy)...\")\n",
                "df_4h['return'] = df_4h['price'].pct_change().fillna(0)\n",
                "df_4h['volatility'] = df_4h['return'].rolling(window=20).std().fillna(0)\n",
                "\n",
                "print(\"Volatility Feature Created.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Selected Features: ['return', 'volume', 'Sentiment Score', 'volatility']\n",
                        "Scaling data with RobustScaler...\n",
                        "Generating sequences: Input=30, Output=7\n",
                        "Train shapes: X=(456, 30, 4), y=(456, 7)\n",
                        "Test shapes: X=(123, 30, 4), y=(123, 7)\n",
                        "Saved daily_data.npz (Fixed Version: No Leakage + RobustScaler)\n"
                    ]
                }
            ],
            "source": [
                "# Feature Engineering\n",
                "# Features: Return, Volume, Sentiment, Volatility\n",
                "feature_cols = ['return', 'volume', 'Sentiment Score', 'volatility']\n",
                "target_col = 'return'\n",
                "\n",
                "print(f\"Selected Features: {feature_cols}\")\n",
                "\n",
                "split_idx = int(len(df_4h) * 0.8)\n",
                "train_df = df_4h.iloc[:split_idx]\n",
                "test_df = df_4h.iloc[split_idx:]\n",
                "\n",
                "# Scalers (Updated to RobustScaler for Financial Data to handle outliers)\n",
                "print(\"Scaling data with RobustScaler...\")\n",
                "f_scaler = RobustScaler()\n",
                "t_scaler = RobustScaler()\n",
                "\n",
                "# Fit on TRAIN only\n",
                "X_train_s = f_scaler.fit_transform(train_df[feature_cols])\n",
                "# Transform TEST\n",
                "X_test_s = f_scaler.transform(test_df[feature_cols])\n",
                "\n",
                "# Fit Target on TRAIN only\n",
                "y_train_s = t_scaler.fit_transform(train_df[[target_col]])\n",
                "y_test_s = t_scaler.transform(test_df[[target_col]])\n",
                "\n",
                "print(f\"Generating sequences: Input={N_STEPS_IN}, Output={N_STEPS_OUT}\")\n",
                "\n",
                "X_train, y_train = create_multistep_sequences(X_train_s, y_train_s, N_STEPS_IN, N_STEPS_OUT)\n",
                "\n",
                "# Combined for test extraction (sliding window over boundary)\n",
                "full_inputs = np.vstack([X_train_s, X_test_s])\n",
                "full_targets = np.vstack([y_train_s, y_test_s])\n",
                "X_all, y_all = create_multistep_sequences(full_inputs, full_targets, N_STEPS_IN, N_STEPS_OUT)\n",
                "\n",
                "# Extract Test portion\n",
                "X_test = X_all[len(X_train):]\n",
                "y_test = y_all[len(y_train):]\n",
                "\n",
                "print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n",
                "print(f\"Test shapes: X={X_test.shape}, y={y_test.shape}\")\n",
                "\n",
                "# Save\n",
                "dates = df_4h.index.astype(str).tolist()\n",
                "\n",
                "np.savez('daily_data.npz', \n",
                "            X_train=X_train, y_train=y_train,\n",
                "            X_test=X_test, y_test=y_test,\n",
                "            dates=dates,\n",
                "            n_steps_in=N_STEPS_IN, n_steps_out=N_STEPS_OUT)\n",
                "\n",
                "joblib.dump(f_scaler, 'feature_scaler.pkl')\n",
                "joblib.dump(t_scaler, 'target_scaler_daily.pkl')\n",
                "print(\"Saved daily_data.npz (Fixed Version: No Leakage + RobustScaler)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
